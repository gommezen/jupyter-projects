{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Market analysis with Jupyter AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# üìä Market behavior of Palantir Technologies with local AI tools\n",
    "# As of November 7, 2025 ‚Ä¢ 10:00 PM GMT+1\n",
    "# -------------------------------------------------------------\n",
    "# Requirements:\n",
    "#   pip install yfinance pandas numpy requests matplotlib\n",
    "#   (ensure your local Ollama model endpoint is running)\n",
    "# -------------------------------------------------------------\n",
    "# adapted from DeepLearningAI\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1. Defining dates and ticker values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üéØ Define analysis parameters\n",
    "# --------------------------------------------------------------\n",
    "# This cell sets the scope of the financial analysis by specifying\n",
    "# the stock ticker symbol and the time range of interest.\n",
    "# These variables are used throughout the notebook to download,\n",
    "# analyze, and visualize Palantir‚Äôs market data for the period\n",
    "# January 1, 2023 ‚Üí November 7, 2025.\n",
    "\n",
    "ticker = \"PLTR\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2025-11-07\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2. Download stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "To download Palentir stock data, use a prompt like this:\n",
    "\n",
    "    Use yfinance to download Palentir (PLTR) stock data for this period:\n",
    "\n",
    "- start date: January 01, 2023\n",
    "- end date: November 07, 2025\n",
    "- Save the returned results in a DataFrame called PLTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üì• Fetch and inspect historical market data\n",
    "# --------------------------------------------------------------\n",
    "# This cell retrieves Palantir's historical stock data from Yahoo Finance\n",
    "# using the yfinance API for the specified date range.\n",
    "# The resulting DataFrame (`pltr`) contains daily Open, High, Low, Close, \n",
    "# and Volume data. We preview the first and last rows to ensure that\n",
    "# the data download completed successfully and the date range is correct.\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "# Download stock data for Palantir\n",
    "pltr = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(\"First few rows:\")\n",
    "print(pltr.head())\n",
    "\n",
    "# Display the last few rows to confirm\n",
    "print(\"\\nLast few rows:\")\n",
    "print(pltr.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "In the chat window, ask how you can flatten the columns of the DataFrame using a prompt like this:\n",
    "> The DataFrame aapl has multiIndexed columns [('Close','PLTR'),('High','PLTR'),('Low','PLTR'),('Open','PLTR'),('Volume','PLTR')]. Flatten the columns by removing 'PLTR'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üß± Flatten column structure for easier access\n",
    "# --------------------------------------------------------------\n",
    "# When data is downloaded for multiple tickers, yfinance often returns\n",
    "# a DataFrame with a MultiIndex (e.g., ('Close', 'PLTR')). \n",
    "# This step simplifies the structure by keeping only the first-level \n",
    "# column names ('Close', 'Open', 'High', etc.), making the dataset \n",
    "# easier to manipulate and reference in subsequent analysis.\n",
    "# After flattening, we print the column names and a few rows to confirm.\n",
    "\n",
    "pltr.columns = pltr.columns.get_level_values(0)\n",
    "\n",
    "# Verify the change\n",
    "print(pltr.columns)\n",
    "print(pltr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Basic Statistics & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "In the same chat window, use a prompt like this to calculate the basic descriptive statistics of the DataFrame:\n",
    "> Display the shape and statistical summary of the DataFrame pltr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üìä Inspect dataset structure and key statistics\n",
    "# --------------------------------------------------------------\n",
    "# This cell provides a quick overview of the dataset after loading and cleaning.\n",
    "# The shape reveals how many observations (rows) and variables (columns) \n",
    "# the DataFrame contains, helping to confirm that the data covers \n",
    "# the expected period and fields. \n",
    "# The descriptive summary offers insight into the central tendencies,\n",
    "# dispersion, and ranges of numerical variables before deeper analysis.\n",
    "\n",
    "# Display the shape (rows, columns)\n",
    "print(\"Shape of the DataFrame/pltr:\", pltr.shape)\n",
    "\n",
    "# Display the statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(pltr.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "To calculate the total return, use a prompt like this:\n",
    "> Use the Close column of DataFrame pltr to find the total return in percentage (total_return) based on the start price and end price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üí∞ Calculate total investment return over the analysis period\n",
    "# --------------------------------------------------------------\n",
    "# This cell computes Palantir‚Äôs total percentage return based on the \n",
    "# closing prices at the start and end of the selected period. \n",
    "# The calculation provides a single performance metric summarizing \n",
    "# overall price growth (or decline) from January 2023 to November 2025, \n",
    "# serving as a baseline for further volatility and trend analysis.\n",
    "\n",
    "# Getting starting and ending prices\n",
    "start_price = pltr['Close'].iloc[0]\n",
    "end_price = pltr['Close'].iloc[-1]\n",
    "\n",
    "#calculate total return\n",
    "total_return = ((end_price - start_price) / start_price) * 100\n",
    "\n",
    "print(f\"Total return: {total_return:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the Closing Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Create a line chart showing the closing price trend using the column 'Close' of the DataFrame pltr.\n",
    "\n",
    "> Use matplotlib to create a professional-looking chart with:\n",
    "> - Clear title and axis labels\n",
    "> - Grid for readability\n",
    "> - Appropriate colors and styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üìà Visualize closing price trend over time\n",
    "# --------------------------------------------------------------\n",
    "# This cell generates a clean, time-series chart of Palantir‚Äôs closing prices\n",
    "# across the selected period. The visualization highlights overall price \n",
    "# movement and trend direction, serving as an intuitive validation of \n",
    "# the computed performance metrics.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Plot the close price trend\n",
    "plt.plot(pltr['Close'], color='teal', linewidth=2, label='Closing Price')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('PLTR Closing Price Trend', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Closing Price (USD)', fontsize=12)\n",
    "\n",
    "# Add grid for readability\n",
    "plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add tight layout for aesthetics\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "- To find the dates that correspond to the peak and lowest prices, use a prompt like this:\n",
    "  > Use the Close column of aapl dataFrame to find and print:\n",
    "  > - the peak date (in a variable called `peak_date`) that corresponds to the maximum closing price `peak_price`\n",
    "  > - the lowest date (in a variable called `lowest_date`) that corresponds to the minimum closing price `lowest_price`\n",
    "  >\n",
    "  > Update the above code to show the peak and low prices in the line chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üìç Identify and visualize price extremes within the analysis period\n",
    "# --------------------------------------------------------------\n",
    "# This cell detects Palantir‚Äôs highest and lowest closing prices \n",
    "# and their corresponding dates within the selected timeframe. \n",
    "# It then visualizes these key inflection points on the price trend \n",
    "# chart to reveal market extremes and potential turning points. \n",
    "# Annotating these values helps verify data integrity and supports \n",
    "# later contextual analysis (e.g., linking peaks/lows to major news events).\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Find the peak (maximum) and lowest (minimum) closing prices\n",
    "peak_price = pltr['Close'].max()\n",
    "lowest_price = pltr['Close'].min()\n",
    "\n",
    "# Find the corresponding dates\n",
    "peak_date = pltr['Close'].idxmax()\n",
    "lowest_date = pltr['Close'].idxmin()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Peak price: {peak_price:.2f} on {peak_date}\")\n",
    "print(f\"Lowest price: {lowest_price:.2f} on {lowest_date}\")\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the closing price trend\n",
    "plt.plot(pltr['Close'], color='navy', linewidth=2, label='Closing Price')\n",
    "\n",
    "# Highlight the peak and lowest points\n",
    "plt.scatter(peak_date, peak_price, color='green', s=100, label='Peak Price', zorder=5)\n",
    "plt.scatter(lowest_date, lowest_price, color='red', s=100, label='Lowest Price', zorder=5)\n",
    "\n",
    "# Annotate the points\n",
    "plt.annotate(f'Peak: {peak_price:.2f}', \n",
    "             xy=(peak_date, peak_price), \n",
    "             xytext=(peak_date, peak_price * 1.02),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05),\n",
    "             fontsize=10, color='green')\n",
    "\n",
    "plt.annotate(f'Low: {lowest_price:.2f}', \n",
    "             xy=(lowest_date, lowest_price), \n",
    "             xytext=(lowest_date, lowest_price * 0.95),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=10, color='red')\n",
    "\n",
    "# Title, labels, grid, legend, and layout\n",
    "plt.title('PLTR Closing Price Trend with Peak and Low Points', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Closing Price (USD)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "- To find the context related to the peak and lowest date, use a prompt like this:\n",
    "  > For the `peak_date` and `lowest_date`, search for related Palentir news using Serper. The Serper API key is saved in a .env file. Store the snippets of the found articles in a json string `news_snippets` that has these fields: peak_date, lowest_date, peak_news_snippets, lowest_news_snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üì∞ Retrieve contextual financial news for identified price events\n",
    "# --------------------------------------------------------------\n",
    "# This cell connects to the Serper API to collect recent news headlines\n",
    "# related to Palantir Technologies around its detected peak and low dates. \n",
    "# By enriching quantitative findings with relevant qualitative context,\n",
    "# the analysis can link market movements to real-world events such as \n",
    "# earnings reports, contracts, or investor sentiment shifts. \n",
    "# The results are structured as a JSON string for seamless integration \n",
    "# into later reporting and summarization steps.\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load Serper API key from .env file\n",
    "load_dotenv()\n",
    "serper_api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "# Define a helper function to search news via Serper\n",
    "def search_news(query):\n",
    "    url = \"https://google.serper.dev/news\"\n",
    "    headers = {\n",
    "        \"X-API-KEY\": serper_api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"q\": query, \"num\": 5}  # limit to top 5 relevant articles\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [item[\"snippet\"] for item in data.get(\"news\", [])]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Build search queries for Palantir around peak/low dates\n",
    "peak_query = f\"Palantir Technologies news {peak_date}\"\n",
    "lowest_query = f\"Palantir Technologies news {lowest_date}\"\n",
    "\n",
    "# Fetch news snippets\n",
    "peak_news_snippets = search_news(peak_query)\n",
    "lowest_news_snippets = search_news(lowest_query)\n",
    "\n",
    "# Combine into a JSON-formatted string\n",
    "news_snippets = json.dumps({\n",
    "    \"peak_date\": str(peak_date),\n",
    "    \"lowest_date\": str(lowest_date),\n",
    "    \"peak_news_snippets\": peak_news_snippets,\n",
    "    \"lowest_news_snippets\": lowest_news_snippets\n",
    "}, indent=4)\n",
    "\n",
    "print(news_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "- To calculate the signal's volatility, use a prompt like this: \n",
    "  >In the DataFrame pltr, find the overall volatility in percentage using the column Close. Volatility is the standard deviation of the daily percentage changes. Save the result in a variable called `volatility`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üìä Calculate overall price volatility for the analysis period\n",
    "# --------------------------------------------------------------\n",
    "# This cell measures Palantir‚Äôs overall price volatility as the \n",
    "# standard deviation of its daily percentage returns. Volatility \n",
    "# quantifies the degree of price fluctuation over time ‚Äî a key risk \n",
    "# indicator that helps assess the stock‚Äôs stability and sensitivity \n",
    "# to market conditions. The result, expressed as a percentage, \n",
    "# summarizes how much the price typically deviates from its mean.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Compute daily percentage changes\n",
    "daily_returns = pltr['Close'].pct_change()\n",
    "\n",
    "# Calculate volatility as the standard deviation of daily returns (in %)\n",
    "volatility = daily_returns.std() * 100\n",
    "\n",
    "print(f\"Overall Volatility: {volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "- To find and plot the rolling volatility, use a prompt like this:\n",
    "  > Calculate the rolling volatility as the as 20-day standard deviation of the daily percentage change and plot it. Identify days of high volatility where volatility is greater than mean + std. Save the days of high volatility in a DataFrame called `high_vol_days`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# ‚öôÔ∏è Analyze and visualize short-term volatility dynamics\n",
    "# --------------------------------------------------------------\n",
    "# This cell calculates Palantir‚Äôs 20-day rolling volatility, providing a \n",
    "# moving estimate of how much daily returns fluctuate over time. By comparing \n",
    "# each window‚Äôs volatility to its long-term average and standard deviation, \n",
    "# we can flag periods of unusually high market turbulence. The resulting \n",
    "# chart visualizes volatility clusters and potential stress periods ‚Äî useful \n",
    "# for understanding risk patterns, earnings reactions, or market sentiment shifts.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute daily percent change\n",
    "pltr['Daily_Return'] = pltr['Close'].pct_change()\n",
    "\n",
    "# Compute 20-day rolling volatility (in %)\n",
    "pltr['Rolling_Volatility'] = pltr['Daily_Return'].rolling(window=20).std() * 100\n",
    "\n",
    "# Calculate thresholds for high volatility\n",
    "mean_vol = pltr['Rolling_Volatility'].mean()\n",
    "std_vol = pltr['Rolling_Volatility'].std()\n",
    "\n",
    "# Identify days of high volatility\n",
    "high_vol_days = pltr[pltr['Rolling_Volatility'] > mean_vol + std_vol].copy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pltr.index, pltr['Rolling_Volatility'], label='20-Day Rolling Volatility', color='blue')\n",
    "plt.axhline(mean_vol, color='green', linestyle='--', label='Mean Volatility')\n",
    "plt.axhline(mean_vol + std_vol, color='red', linestyle='--', label='Mean + Std (High Vol Threshold)')\n",
    "plt.scatter(high_vol_days.index, high_vol_days['Rolling_Volatility'], color='red', label='High Vol Days', zorder=5)\n",
    "plt.title('Palantir 20-Day Rolling Volatility')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "high_vol_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Step 6: Report Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "- To generate a report summarizing the insights, use a prompt like this:\n",
    "  \n",
    "  > Use locl model: qwen2.5:7b-instruct-q4_0 to generate a summary that takes in these variables:\n",
    "  > - ticker: stock ticker (string)\n",
    "  > - start_date: analysis starting period (string)\n",
    "  > - end_date: analysis end period (string)\n",
    "  > - numerical metrics: total_return & volatility (in percentage)\n",
    "  > - peak_date, peak_price\n",
    "  > - lowest_date, lowest_price\n",
    "  > - high_vol_days: pandas DataFrame showing high volatility days\n",
    "  > - news_snippets: string containing snippet of news for the peak and lowest dates\n",
    "  >\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üß† Generate structured AI-assisted market summary\n",
    "# --------------------------------------------------------------\n",
    "# This cell synthesizes quantitative performance metrics and contextual \n",
    "# financial news into a structured research brief using a local LLM model.\n",
    "# It validates that key indicators (return, volatility, peak/low metrics) \n",
    "# are available or recomputes them if missing, retrieves relevant news via \n",
    "# the Serper API, and builds a detailed analyst-style prompt. \n",
    "# The final summary is generated through a local Ollama model \n",
    "# (e.g., Llama 3.1 or Qwen 2.5) and printed as a professional report.\n",
    "# This step bridges the gap between raw financial analysis and \n",
    "# natural-language interpretation for decision-ready insights.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os, requests, pandas as pd, yfinance as yf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# --- Load environment & configuration ---\n",
    "load_dotenv()\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434/api/chat\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct-q8_0\") #qwen2.5:7b-instruct-q4_0\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "ticker = \"PLTR\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2025-11-07\"\n",
    "\n",
    "# --- Ensure required metrics exist (recompute if needed) ---\n",
    "missing_vars = [v for v in [\"total_return\",\"volatility\",\"peak_price\",\n",
    "                            \"lowest_price\",\"peak_date\",\"lowest_date\",\"high_vol_days\"]\n",
    "                if v not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚öôÔ∏è  Recomputing metrics for {ticker} (missing {', '.join(missing_vars)}) ...\")\n",
    "    pltr = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "    # Ensure the index is datetime\n",
    "    if not isinstance(pltr.index, pd.DatetimeIndex):\n",
    "        pltr.index = pd.to_datetime(pltr.index, errors=\"coerce\")\n",
    "\n",
    "    pltr[\"Daily_Return\"] = pltr[\"Close\"].pct_change()\n",
    "\n",
    "    total_return = (pltr[\"Close\"].iloc[-1] / pltr[\"Close\"].iloc[0] - 1) * 100\n",
    "    volatility = pltr[\"Daily_Return\"].std() * 100\n",
    "    peak_price = pltr[\"Close\"].max()\n",
    "    lowest_price = pltr[\"Close\"].min()\n",
    "\n",
    "    # Extract corresponding dates (safe datetime formatting)\n",
    "    peak_date = pltr[\"Close\"].idxmax()\n",
    "    lowest_date = pltr[\"Close\"].idxmin()\n",
    "    peak_date = peak_date.strftime(\"%Y-%m-%d %H:%M:%S\") if hasattr(peak_date, \"strftime\") else str(peak_date)\n",
    "    lowest_date = lowest_date.strftime(\"%Y-%m-%d %H:%M:%S\") if hasattr(lowest_date, \"strftime\") else str(lowest_date)\n",
    "\n",
    "    # Identify high volatility days\n",
    "    rolling_vol = pltr[\"Daily_Return\"].rolling(20).std() * 100\n",
    "    threshold = rolling_vol.quantile(0.95)\n",
    "    high_vol_days = pltr.loc[rolling_vol > threshold, [\"Close\"]].copy()\n",
    "    high_vol_days[\"volatility\"] = rolling_vol[rolling_vol > threshold]\n",
    "\n",
    "# --- Ensure numeric values are scalars ---\n",
    "def to_scalar(x):\n",
    "    if isinstance(x, (pd.Series, np.ndarray, list)):\n",
    "        return float(np.mean(x))\n",
    "    return float(x)\n",
    "\n",
    "total_return = to_scalar(total_return)\n",
    "volatility = to_scalar(volatility)\n",
    "peak_price = to_scalar(peak_price)\n",
    "lowest_price = to_scalar(lowest_price)\n",
    "\n",
    "# --- Fetch news via Serper ---\n",
    "def get_news_snippets(query, max_results=4):\n",
    "    if not SERPER_API_KEY:\n",
    "        return [\"‚ö†Ô∏è SERPER_API_KEY not found ‚Äî please add to .env.\"]\n",
    "    headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"q\": query, \"type\": \"news\", \"num\": max_results}\n",
    "    try:\n",
    "        r = requests.post(\"https://google.serper.dev/search\", headers=headers, json=payload, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        items = r.json().get(\"news\", [])\n",
    "        return [\n",
    "            f\"- {n.get('title','')} ({n.get('source','')}, {n.get('date','')}) ‚Äî {n.get('snippet','')}\"\n",
    "            for n in items\n",
    "        ] or [\"No relevant news found.\"]\n",
    "    except Exception as e:\n",
    "        return [f\"‚ö†Ô∏è Serper error: {e}\"]\n",
    "\n",
    "# --- Retrieve contextual news ---\n",
    "peak_context = get_news_snippets(\"Palantir Technologies November 2025 earnings\")\n",
    "low_context  = get_news_snippets(\"Palantir Technologies January 2023 stock decline\")\n",
    "news_snippets = \"\\n\".join(peak_context[:3] + low_context[:3])\n",
    "\n",
    "# --- Build LLM prompt ---\n",
    "prompt = f\"\"\"\n",
    "You are a **senior equity market analyst** preparing a brief for an institutional research report. \n",
    "Write a concise, structured summary of this stock‚Äôs performance using a professional, objective tone.\n",
    "\n",
    "**Stock Overview**\n",
    "- **Ticker:** {ticker}\n",
    "- **Analysis Period:** {start_date} ‚Üí {end_date}\n",
    "\n",
    "**Quantitative Highlights**\n",
    "- **Total Return:** {total_return:.2f}%\n",
    "- **Volatility:** {volatility:.2f}%\n",
    "- **Peak:** ${peak_price:.2f} on {peak_date}\n",
    "- **Low:** ${lowest_price:.2f} on {lowest_date}\n",
    "\n",
    "**Market Context (recent headlines)**\n",
    "{news_snippets}\n",
    "\n",
    "**Analyst Tasks**\n",
    "1. Describe the stock‚Äôs overall **trend** and major **inflection points** between {start_date} and {end_date}.\n",
    "2. Identify key **drivers of performance** such as AI contracts, government deals, earnings momentum, or macro events.\n",
    "3. Interpret **volatility and investor sentiment**, linking to notable news or leadership commentary.\n",
    "4. Provide a **balanced forward outlook** for 2026, noting valuation, growth opportunities, and risk factors.\n",
    "5. Maintain a factual, analytical, and neutral tone ‚Äî no hype, no filler.\n",
    "\n",
    "Conclude with a one-sentence **Key Takeaway** summarizing the risk‚Äìreward outlook for investors.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Query local Ollama model ---\n",
    "try:\n",
    "    response = requests.post(\n",
    "        OLLAMA_URL,\n",
    "        json={\"model\": OLLAMA_MODEL, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"stream\": False},\n",
    "        timeout=120,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    summary = (\n",
    "        data.get(\"message\", {}).get(\"content\")\n",
    "        or data.get(\"response\")\n",
    "        or str(data)\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- üìà Professional Analyst Summary ---\\n\")\n",
    "    print(summary.strip())\n",
    "    print(\"\\n--- üß© Structured Insight Complete ---\")\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ö†Ô∏è Ollama request error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypandoc\n",
    "print(pypandoc.get_pandoc_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# üìÑ Generate and export an automated analyst report\n",
    "# --------------------------------------------------------------\n",
    "# This cell compiles the results of the Palantir analysis into a \n",
    "# structured Markdown report and automatically converts it to PDF or DOCX. \n",
    "# It combines key metrics, contextual news, and AI-generated insights \n",
    "# into a well-formatted summary for presentation or archival purposes. \n",
    "# The export process prioritizes PDF output using LaTeX (if available), \n",
    "# but falls back to DOCX for compatibility. This step marks the final \n",
    "# stage of the reporting workflow, transforming analytical results \n",
    "# into a polished deliverable for decision-makers.\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pypandoc\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# --- Values from your analysis step ---\n",
    "ticker = \"PLTR\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2025-11-07\"\n",
    "\n",
    "# Computed metrics (from previous block)\n",
    "# total_return, volatility, peak_price, lowest_price, peak_date, lowest_date, news_snippets, summary\n",
    "\n",
    "# --- Helper for readable dates ---\n",
    "def fmt_date(d):\n",
    "    try:\n",
    "        return str(pd.to_datetime(d).date())\n",
    "    except Exception:\n",
    "        return str(d)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# üßæ Build Markdown report\n",
    "# --------------------------------------------------------------\n",
    "md_report = f\"\"\"\n",
    "# üìà {ticker} ‚Äî Stock Performance Summary  \n",
    "**Period:** {start_date} ‚Üí {end_date}  \n",
    "**Generated:** {datetime.now():%Y-%m-%d %H:%M}  \n",
    "**Data source:** Yahoo Finance + Serper + Ollama  \n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Key Metrics\n",
    "- **Total Return:** {total_return:.2f} %\n",
    "- **Volatility:** {volatility:.2f} %\n",
    "- **Peak:** ${peak_price:.2f} on {fmt_date(peak_date)}\n",
    "- **Low:** ${lowest_price:.2f} on {fmt_date(lowest_date)}\n",
    "\n",
    "---\n",
    "\n",
    "## üì∞ Contextual News\n",
    "{news_snippets}\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Analyst Summary\n",
    "{summary.strip()}\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:**  \n",
    "*{summary.split('Key Takeaway:')[-1].strip() if 'Key Takeaway:' in summary else 'See above for details.'}*\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# üíæ Export Markdown ‚Üí PDF (fallback to DOCX if no LaTeX)\n",
    "# --------------------------------------------------------------\n",
    "output_dir = Path(\"reports\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "md_path = output_dir / f\"{ticker.lower()}_report.md\"\n",
    "pdf_path = output_dir / f\"{ticker.lower()}_report.pdf\"\n",
    "docx_path = output_dir / f\"{ticker.lower()}_report.docx\"\n",
    "\n",
    "md_path.write_text(md_report, encoding=\"utf-8\")\n",
    "\n",
    "# Detect PDF engine\n",
    "has_tex = shutil.which(\"xelatex\") or shutil.which(\"pdflatex\")\n",
    "\n",
    "try:\n",
    "    if has_tex:\n",
    "        pypandoc.convert_text(\n",
    "            md_report,\n",
    "            \"pdf\",\n",
    "            format=\"md\",\n",
    "            outputfile=str(pdf_path),\n",
    "            extra_args=[\"--standalone\", \"--pdf-engine=xelatex\"],\n",
    "        )\n",
    "        print(f\"‚úÖ PDF exported ‚Üí {pdf_path}\")\n",
    "    else:\n",
    "        pypandoc.convert_text(\n",
    "            md_report,\n",
    "            \"docx\",\n",
    "            format=\"md\",\n",
    "            outputfile=str(docx_path),\n",
    "            extra_args=[\"--standalone\"],\n",
    "        )\n",
    "        print(f\"‚ö†Ô∏è No LaTeX engine found ‚Äî exported DOCX instead ‚Üí {docx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Export failed: {e}\")\n",
    "    print(f\"Markdown saved to {md_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Projects (venv)",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
